{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e775500f",
   "metadata": {},
   "source": [
    "## Data preparation and Exploring Teacher Models\n",
    "\n",
    "### To make this as multioutput task creating additional continuous target 'max_loan' based on following rule\n",
    "\n",
    "- **'max_loan' to define maximum amount of eligible loan for 'loan_status' == 1 and assign random values between 90000 to 300000 with interval of 5k based on following rules:**\n",
    "\n",
    "1) Generally loan amount is less for the age < 30 as the percieved risk is high, however there is no direct inverse linear relationship\n",
    "2) For working class for equally educated person following is the order of preference from high to low Federal-gov > State-gov > Local-gov > Private  > Self-emp-not-inc > Without-pay > Never-worked\n",
    "3) naturally education-num has positive co-rrelation with maximum amount of eligible loan.\n",
    "4) Based on the occupation make a good judgement about percieved risk and maximum amount of eligible loan. For  example persons with Exec-managerial, Prof-specialty occupation are eligible for higher loan amount than Sales/Adm-clerical. Sales/Adm-clerical are elible for higher loan than people with blue collar jobs like Machine-op-inspct, Farming-fishing etc. Naturally there is a direct relationship between occupation and education-num.\n",
    "5) relationship, race, sex etc. have no connection with maximum amount of eligible loan. So consider them as no influencer.\n",
    "6) net of capital-gain and capital-loss(capital-gain - capital-loss) has generally +ve  co-rrelation with maximum amount of eligible loan.\n",
    "7) hours-per-week generally have no direct co-rrelation with maximum amount of eligible loan. Generally working for less than 35 hours infers not a full time high paying job. At the same time working more than 50 hours may indicate blue collar jobs and hence also not very high paying. consider this this in conjunction with the occupation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30c26db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fairlearn.datasets import fetch_adult\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabpfn import TabPFNRegressor\n",
    "\n",
    "regressor_model = TabPFNRegressor()\n",
    "import numpy as np\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58ec1d4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  workclass  fnlwgt     education  education-num      marital-status  \\\n",
      "0   25    Private  226802          11th              7       Never-married   \n",
      "1   38    Private   89814       HS-grad              9  Married-civ-spouse   \n",
      "2   28  Local-gov  336951    Assoc-acdm             12  Married-civ-spouse   \n",
      "3   44    Private  160323  Some-college             10  Married-civ-spouse   \n",
      "4   18        NaN  103497  Some-college             10       Never-married   \n",
      "\n",
      "          occupation relationship   race     sex  capital-gain  capital-loss  \\\n",
      "0  Machine-op-inspct    Own-child  Black    Male             0             0   \n",
      "1    Farming-fishing      Husband  White    Male             0             0   \n",
      "2    Protective-serv      Husband  White    Male             0             0   \n",
      "3  Machine-op-inspct      Husband  Black    Male          7688             0   \n",
      "4                NaN    Own-child  White  Female             0             0   \n",
      "\n",
      "   hours-per-week native-country loan_status  max_loan  \n",
      "0              40  United-States           0         0  \n",
      "1              50  United-States           0         0  \n",
      "2              40  United-States           1    265000  \n",
      "3              40  United-States           1    195000  \n",
      "4              30  United-States           0         0  \n",
      "\n",
      "Loan statistics for eligible applicants:\n",
      "count     11687.000000\n",
      "mean     212789.851972\n",
      "std       71080.877706\n",
      "min       90000.000000\n",
      "25%      150000.000000\n",
      "50%      215000.000000\n",
      "75%      295000.000000\n",
      "max      300000.000000\n",
      "Name: max_loan, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/yn/7tg__cr53l12g66q89c8fgtr0000gn/T/ipykernel_48961/568516613.py:8: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  data.target.replace({ \"<=50K\": 0, \">50K\": 1 }, inplace=True)\n",
      "/var/folders/yn/7tg__cr53l12g66q89c8fgtr0000gn/T/ipykernel_48961/568516613.py:8: FutureWarning: The behavior of Series.replace (and DataFrame.replace) with CategoricalDtype is deprecated. In a future version, replace will only be used for cases that preserve the categories. To change the categories, use ser.cat.rename_categories instead.\n",
      "  data.target.replace({ \"<=50K\": 0, \">50K\": 1 }, inplace=True)\n"
     ]
    }
   ],
   "source": [
    "def getDataset():\n",
    "    data = fetch_adult(as_frame=True)\n",
    "    df = data.data\n",
    "    data.target.replace({ \"<=50K\": 0, \">50K\": 1 }, inplace=True)\n",
    "    df['loan_status'] = data.target\n",
    "    \n",
    "    # Create max_loan column only for loan_status == 1\n",
    "    df['max_loan'] = 0\n",
    "    \n",
    "    # Filter for eligible loan applicants (loan_status == 1)\n",
    "    eligible_mask = df['loan_status'] == 1\n",
    "    eligible_df = df[eligible_mask].copy()\n",
    "    \n",
    "    if len(eligible_df) > 0:\n",
    "        # Set random seed for reproducibility\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Initialize base loan amounts (90k to 300k in 5k intervals)\n",
    "        loan_amounts = np.arange(90000, 305000, 5000)\n",
    "        base_loans = np.random.choice(loan_amounts, size=len(eligible_df))\n",
    "        \n",
    "        # Apply adjustments based on various factors\n",
    "        loan_adjustments = np.ones(len(eligible_df))\n",
    "        \n",
    "        # 1. Age factor (age < 30 gets lower amounts, but not strictly linear)\n",
    "        age_factor = np.where(eligible_df['age'] < 30, \n",
    "                             np.random.uniform(0.7, 0.9, size=len(eligible_df)),\n",
    "                             np.where(eligible_df['age'] > 50,\n",
    "                                     np.random.uniform(0.9, 1.1, size=len(eligible_df)),\n",
    "                                     np.random.uniform(0.85, 1.05, size=len(eligible_df))))\n",
    "        loan_adjustments *= age_factor\n",
    "        \n",
    "        # 2. Workclass factor (Federal-gov > State-gov > Local-gov > Private > Self-emp-not-inc > Without-pay > Never-worked)\n",
    "        workclass_multipliers = {\n",
    "            'Federal-gov': 1.2,\n",
    "            'State-gov': 1.15,\n",
    "            'Local-gov': 1.1,\n",
    "            'Private': 1.0,\n",
    "            'Self-emp-inc': 0.95,\n",
    "            'Self-emp-not-inc': 0.85,\n",
    "            'Without-pay': 0.6,\n",
    "            'Never-worked': 0.5\n",
    "        }\n",
    "        # Handle categorical workclass properly\n",
    "        workclass_factor = []\n",
    "        for wc in eligible_df['workclass']:\n",
    "            if pd.isna(wc) or wc not in workclass_multipliers:\n",
    "                workclass_factor.append(0.8)  # Default for unknown/missing workclass\n",
    "            else:\n",
    "                workclass_factor.append(workclass_multipliers[wc])\n",
    "        workclass_factor = np.array(workclass_factor)\n",
    "        loan_adjustments *= workclass_factor\n",
    "        \n",
    "        # 3. Education factor (positive correlation with education-num)\n",
    "        education_factor = 0.7 + (eligible_df['education-num'] / 16) * 0.6  # Scale from 0.7 to 1.3\n",
    "        loan_adjustments *= education_factor\n",
    "        \n",
    "        # 4. Occupation factor (risk-based assessment)\n",
    "        occupation_multipliers = {\n",
    "            'Exec-managerial': 1.3,\n",
    "            'Prof-specialty': 1.25,\n",
    "            'Tech-support': 1.1,\n",
    "            'Sales': 1.0,\n",
    "            'Adm-clerical': 0.95,\n",
    "            'Protective-serv': 0.9,\n",
    "            'Craft-repair': 0.85,\n",
    "            'Transport-moving': 0.8,\n",
    "            'Machine-op-inspct': 0.75,\n",
    "            'Other-service': 0.7,\n",
    "            'Farming-fishing': 0.65,\n",
    "            'Handlers-cleaners': 0.6,\n",
    "            'Priv-house-serv': 0.55,\n",
    "            'Armed-Forces': 1.05\n",
    "        }\n",
    "        # Handle categorical occupation properly\n",
    "        occupation_factor = []\n",
    "        for occ in eligible_df['occupation']:\n",
    "            if pd.isna(occ) or occ not in occupation_multipliers:\n",
    "                occupation_factor.append(0.8)  # Default for unknown/missing occupation\n",
    "            else:\n",
    "                occupation_factor.append(occupation_multipliers[occ])\n",
    "        occupation_factor = np.array(occupation_factor)\n",
    "        loan_adjustments *= occupation_factor\n",
    "        \n",
    "        # 6. Capital gain/loss factor (net capital has positive correlation)\n",
    "        net_capital = eligible_df['capital-gain'] - eligible_df['capital-loss']\n",
    "        # Normalize capital gains impact (cap the effect to avoid extreme values)\n",
    "        capital_factor = 1.0 + np.clip(net_capital / 100000, -0.2, 0.3)\n",
    "        loan_adjustments *= capital_factor\n",
    "        \n",
    "        # 7. Hours per week factor (sweet spot around 40-50 hours)\n",
    "        hours_factor = np.where(eligible_df['hours-per-week'] < 35, 0.85,\n",
    "                               np.where(eligible_df['hours-per-week'] > 50, 0.9, 1.0))\n",
    "        loan_adjustments *= hours_factor\n",
    "        \n",
    "        # Apply all adjustments to base loan amounts\n",
    "        final_loans = base_loans * loan_adjustments\n",
    "        \n",
    "        # Round to nearest 5000 and ensure within bounds\n",
    "        final_loans = np.round(final_loans / 5000) * 5000\n",
    "        final_loans = np.clip(final_loans, 90000, 300000)\n",
    "        \n",
    "        # Assign the calculated loan amounts back to the main dataframe\n",
    "        df.loc[eligible_mask, 'max_loan'] = final_loans.astype(int)\n",
    "    \n",
    "    return df\n",
    "\n",
    "df = pd.DataFrame(getDataset()) \n",
    "print(df.head())\n",
    "print(f\"\\nLoan statistics for eligible applicants:\")\n",
    "print(df[df['loan_status'] == 1]['max_loan'].describe())\n",
    "df.to_csv('../data/loan_data_with_max_loan.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a423e31d",
   "metadata": {},
   "source": [
    "### 🧠 About TabPFN Foundation Model:\n",
    "\n",
    "A substantial foundation model with over 11 million parameters, trained on diverse structured data to learn general patterns that transfer to new tabular datasets both for regression and classification tasks!\n",
    "\n",
    "- **Pre-trained** on millions of synthetic tabular datasets\n",
    "- **Transformer-based** architecture optimized for tabular data\n",
    "- **Foundation model** that can adapt to new tasks with minimal data\n",
    "- **Ensemble approach** uses multiple models for robust predictions\n",
    "- **Efficient** for small datasets (≤10K samples)\n",
    "\n",
    "- **More details is here :**\n",
    "https://github.com/PriorLabs/TabPFN\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3b7d0cc",
   "metadata": {},
   "source": [
    "- **Sneak peek Teacher Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c77739a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating dummy data and fitting the model...\n",
      "Model fitted! Now analyzing the TabPFN model parameters...\n",
      "\n",
      "============================================================\n",
      "TABPFN REGRESSOR MODEL ANALYSIS\n",
      "============================================================\n",
      " Model Architecture: PerFeatureTransformer\n",
      " Total Parameters: 11,081,864\n",
      " Trainable Parameters: 11,081,864\n",
      " Model Size: 42.27 MB\n",
      "\n",
      " Ensemble Configuration:\n",
      "   Number of estimators: 8\n",
      "   Total ensemble parameters: 88,654,912\n",
      "\n",
      " Key Facts about TabPFN:\n",
      "   • Foundation model trained on millions of synthetic datasets\n",
      "   • Uses Per-Feature Transformer architecture\n",
      "   • Optimized for small tabular datasets (≤10K samples)\n",
      "   • Each model in ensemble has ~11M parameters\n",
      "   • Default ensemble uses 8 models for robust predictions\n",
      "\n",
      " Parameter Distribution:\n",
      "   transformer_encoder: 7,077,888 (63.9%)\n",
      "   decoder_dict: 3,993,224 (36.0%)\n",
      "   feature_positional_embedding_embeddings: 9,408 (0.1%)\n",
      "   encoder: 768 (0.0%)\n",
      "   y_encoder: 576 (0.0%)\n",
      "Model fitted! Now analyzing the TabPFN model parameters...\n",
      "\n",
      "============================================================\n",
      "TABPFN REGRESSOR MODEL ANALYSIS\n",
      "============================================================\n",
      " Model Architecture: PerFeatureTransformer\n",
      " Total Parameters: 11,081,864\n",
      " Trainable Parameters: 11,081,864\n",
      " Model Size: 42.27 MB\n",
      "\n",
      " Ensemble Configuration:\n",
      "   Number of estimators: 8\n",
      "   Total ensemble parameters: 88,654,912\n",
      "\n",
      " Key Facts about TabPFN:\n",
      "   • Foundation model trained on millions of synthetic datasets\n",
      "   • Uses Per-Feature Transformer architecture\n",
      "   • Optimized for small tabular datasets (≤10K samples)\n",
      "   • Each model in ensemble has ~11M parameters\n",
      "   • Default ensemble uses 8 models for robust predictions\n",
      "\n",
      " Parameter Distribution:\n",
      "   transformer_encoder: 7,077,888 (63.9%)\n",
      "   decoder_dict: 3,993,224 (36.0%)\n",
      "   feature_positional_embedding_embeddings: 9,408 (0.1%)\n",
      "   encoder: 768 (0.0%)\n",
      "   y_encoder: 576 (0.0%)\n"
     ]
    }
   ],
   "source": [
    "# Create dummy data to fit the model (this will load the underlying PyTorch model)\n",
    "print(\"Creating dummy data and fitting the model...\")\n",
    "X_dummy, y_dummy = make_regression(n_samples=100, n_features=5, random_state=42)\n",
    "\n",
    "# Fit the model - this will load the underlying PyTorch model\n",
    "regressor_model.fit(X_dummy, y_dummy)\n",
    "\n",
    "print(\"Model fitted! Now analyzing the TabPFN model parameters...\")\n",
    "\n",
    "# Get the underlying PyTorch model\n",
    "model = regressor_model.model_\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"TABPFN REGRESSOR MODEL ANALYSIS\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "print(f\" Model Architecture: {model.__class__.__name__}\")\n",
    "print(f\" Total Parameters: {total_params:,}\")\n",
    "print(f\" Trainable Parameters: {trainable_params:,}\")\n",
    "print(f\" Model Size: {(total_params * 4) / (1024**2):.2f} MB\")\n",
    "\n",
    "print(f\"\\n Ensemble Configuration:\")\n",
    "print(f\"   Number of estimators: {regressor_model.n_estimators}\")\n",
    "print(f\"   Total ensemble parameters: {total_params * regressor_model.n_estimators:,}\")\n",
    "\n",
    "# Show parameter distribution by layer type\n",
    "layer_counts = {}\n",
    "for name, param in model.named_parameters():\n",
    "    layer_type = name.split('.')[0]\n",
    "    layer_counts[layer_type] = layer_counts.get(layer_type, 0) + param.numel()\n",
    "\n",
    "print(f\"\\n Parameter Distribution:\")\n",
    "for layer_type, count in sorted(layer_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "    percentage = (count / total_params) * 100\n",
    "    print(f\"   {layer_type}: {count:,} ({percentage:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ac0353",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Getting TabPFN parameter information...\n",
      "\n",
      "📊 TABPFN PARAMETER SUMMARY\n",
      "==================================================\n",
      "Regressor Parameters: 11,081,864\n",
      "Classifier Parameters: 7,244,554\n",
      "Architecture: PerFeatureTransformer\n",
      "Single Model Size: 42.3 MB\n",
      "Default Ensemble Size: 8 models\n",
      "Total Ensemble Parameters: 88,654,912\n",
      "\n",
      "💡 Answer to your question:\n",
      "The TabPFN foundation model has 11,081,864 parameters\n",
      "When used as an ensemble (default), it uses 88,654,912 total parameters\n"
     ]
    }
   ],
   "source": [
    "# Utility function to get TabPFN parameter count\n",
    "def get_tabpfn_parameter_count(model_type='regressor'):\n",
    "    \"\"\"\n",
    "    Get the parameter count of a TabPFN model without fitting data.\n",
    "    \n",
    "    Args:\n",
    "        model_type (str): 'regressor' or 'classifier'\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing parameter information\n",
    "    \"\"\"\n",
    "    from tabpfn import TabPFNRegressor, TabPFNClassifier\n",
    "    from sklearn.datasets import make_regression, make_classification\n",
    "    \n",
    "    # Choose the appropriate model and dummy data\n",
    "    if model_type.lower() == 'regressor':\n",
    "        model = TabPFNRegressor()\n",
    "        X_dummy, y_dummy = make_regression(n_samples=50, n_features=5, random_state=42)\n",
    "    else:\n",
    "        model = TabPFNClassifier()\n",
    "        X_dummy, y_dummy = make_classification(n_samples=50, n_features=5, n_classes=2, random_state=42)\n",
    "    \n",
    "    # Fit with minimal data to load the model\n",
    "    model.fit(X_dummy, y_dummy)\n",
    "    \n",
    "    # Get parameter count\n",
    "    pytorch_model = model.model_\n",
    "    total_params = sum(p.numel() for p in pytorch_model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in pytorch_model.parameters() if p.requires_grad)\n",
    "    \n",
    "    return {\n",
    "        'model_type': model_type,\n",
    "        'architecture': pytorch_model.__class__.__name__,\n",
    "        'total_parameters': total_params,\n",
    "        'trainable_parameters': trainable_params,\n",
    "        'model_size_mb': (total_params * 4) / (1024**2),\n",
    "        'n_estimators': model.n_estimators,\n",
    "        'ensemble_total_params': total_params * model.n_estimators\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "print(\"Getting TabPFN parameter information...\")\n",
    "regressor_info = get_tabpfn_parameter_count('regressor')\n",
    "classifier_info = get_tabpfn_parameter_count('classifier')\n",
    "\n",
    "print(\"\\n TABPFN PARAMETER SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Regressor Parameters: {regressor_info['total_parameters']:,}\")\n",
    "print(f\"Classifier Parameters: {classifier_info['total_parameters']:,}\")\n",
    "print(f\"Architecture: {regressor_info['architecture']}\")\n",
    "print(f\"Single Model Size: {regressor_info['model_size_mb']:.1f} MB\")\n",
    "print(f\"Default Ensemble Size: {regressor_info['n_estimators']} models\")\n",
    "print(f\"Total Ensemble Parameters: {regressor_info['ensemble_total_params']:,}\")\n",
    "\n",
    "print(f\"\\n Answer to your question:\")\n",
    "print(f\"The TabPFN foundation model has {regressor_info['total_parameters']:,} parameters\")\n",
    "print(f\"When used as an ensemble (default), it uses {regressor_info['ensemble_total_params']:,} total parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "923b993d",
   "metadata": {},
   "source": [
    "## 🔍 TabPFN Model Parameters - Summary\n",
    "\n",
    "\n",
    "### 📊 Key Findings:\n",
    "\n",
    "1. **TabPFNRegressor**: **11,081,864 parameters** (~11.1M)\n",
    "2. **TabPFNClassifier**: **7,244,554 parameters** (~7.2M) \n",
    "3. **Architecture**: PerFeatureTransformer (Transformer-based)\n",
    "4. **Model Size**: ~42.3 MB per model\n",
    "5. **Ensemble**: 8 models by default = **88,654,912 total parameters**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ee314d",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-distillation",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
